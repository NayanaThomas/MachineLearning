{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#required python packages are imported\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read train data from the csv file\n",
    "training_dataset = pd.read_csv(\"textTrainData.txt\", sep=\"\\t\", header=0, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we are splitting the text based on the sentiment\n",
    "def getTextForSentiment(sentence, sentiment):\n",
    "    # Join together the text in the reviews for a particular sentiment.\n",
    "    # We lowercase to avoid \"Not\" and \"not\" being seen as different words, for example.\n",
    "    s=\"\"\n",
    "    for index, row in sentence.iterrows():\n",
    "        if(row[\"Sentiment\"]==sentiment):\n",
    "            s=s+row[\"Sentence\"].lower()\n",
    "    return s        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we are splitting dataset to positive and negative samples\n",
    "positive_sample = getTextForSentiment(training_dataset, 1)\n",
    "negative_sample = getTextForSentiment(training_dataset, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we are defining a function that will count the number of words for each sample\n",
    "def countWords(Sentence):\n",
    "    words = re.split(\"\\s+\", Sentence)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we are counting the number of words in the positive and negative samples in the dataset\n",
    "count_positive = countWords(positive_sample)\n",
    "count_negative = countWords(negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#next method will calculate the number of samples with the given sentiment\n",
    "def countSampleForSentiment(sentiment):\n",
    "    c=0\n",
    "    for index, row in training_dataset.iterrows():\n",
    "        if(row[\"Sentiment\"]==sentiment):\n",
    "            c+=1\n",
    "    return c        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counting the number of samples with both sentiment values\n",
    "positive_sample_count = countSampleForSentiment(1)\n",
    "negative_sample_count = countSampleForSentiment(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we are calculating the probabilities of positive and negative samples\n",
    "positive_prob = positive_sample_count/len(training_dataset)\n",
    "negative_prob = negative_sample_count/len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we are creating a function that will calculate the proability for a sample sentence\n",
    "def make_class_prediction(text, counts, class_prob, class_count):\n",
    "  prediction = 1\n",
    "  text_counts = Counter(re.split(\"\\s+\", text))\n",
    "  for word in text_counts:\n",
    "      # For every word in the text, we get the number of times that word occured in the reviews for a given class, add 1 to smooth the value, and divide by the total number of words in the class (plus the class_count to also smooth the denominator).\n",
    "      # Smoothing ensures that we don't multiply the prediction by 0 if the word didn't exist in the training data.\n",
    "      # We also smooth the denominator counts to keep things even.\n",
    "      prediction *=  text_counts.get(word) * ((counts.get(word, 0) + 1) / (sum(counts.values()) + class_count))\n",
    "  # Now we multiply by the probability of the class existing in the documents.\n",
    "  return prediction * class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_decision(text, make_class_prediction):\n",
    "    # Compute the negative and positive probabilities.\n",
    "    negative_prediction = make_class_prediction(text, count_negative, negative_prob, negative_sample_count)\n",
    "    positive_prediction = make_class_prediction(text, count_positive, positive_prob, positive_sample_count)\n",
    "    # We assign a classification based on which probability is greater.\n",
    "    if negative_prediction > positive_prediction:\n",
    "      return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicting the target values for the Sentence using the given classifier\n",
    "training_predictions = [make_decision(row['Sentence'], make_class_prediction) for index,row in training_dataset[0:len(training_dataset)].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual target attribute values for training data are retrieved here\n",
    "training_actual = training_dataset['Sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9054\n"
     ]
    }
   ],
   "source": [
    "#we are calculating the accuracy for the training data here\n",
    "training_accuracy = sum(1 for i in range(len(training_predictions)) if training_predictions[i] == training_actual[i]) / float(len(training_predictions))\n",
    "print(\"Training Accuracy: {0:.4f}\".format(training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix: \n",
      " [[1245  135]\n",
      " [ 123 1223]]\n"
     ]
    }
   ],
   "source": [
    "#we are calculating the confusion matrix for the training data below\n",
    "from sklearn.metrics import confusion_matrix\n",
    "training_confusion_matrix = confusion_matrix(training_actual, training_predictions)\n",
    "print(\"Training Confusion Matrix: \\n\", training_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are reading the test data here\n",
    "test_dataset = pd.read_csv(\"textTestData.txt\", sep=\"\\t\", header=0, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the target values for the Sentence using the given classifier for test data\n",
    "test_predictions = [make_decision(row['Sentence'], make_class_prediction) for index,row in test_dataset[0:len(test_dataset)].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#actual target attribute values for test data are retrieved here\n",
    "test_actual = test_dataset['Sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9054\n"
     ]
    }
   ],
   "source": [
    "#we are calculating the accuracy for the test data here\n",
    "test_accuracy = sum(1 for i in range(len(test_predictions)) if test_predictions[i] == test_actual[i]) / float(len(test_predictions))\n",
    "print(\"Test Accuracy: {0:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix: \n",
      " [[1245  135]\n",
      " [ 123 1223]]\n"
     ]
    }
   ],
   "source": [
    "#we are calculating the confusion matrix for the test data below\n",
    "test_confusion_matrix = confusion_matrix(test_actual, test_predictions)\n",
    "print(\"Training Confusion Matrix: \\n\", test_confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
